{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Handwritten Text Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yazeedMohi/Handwritten-Text-Recognition-Arabic-and-English-/blob/master/Handwritten_Text_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9pOwx9T8dc9A"
      },
      "source": [
        "# **Handwritten Arabic text recognition**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJl83eCG_tdL",
        "colab_type": "text"
      },
      "source": [
        "## Configuring Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MoxbE4NReDz1",
        "outputId": "15490c93-7566-43e6-ec8c-22ee99ec5a7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%tensorflow_version 2.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYUWgl-6_td0",
        "colab_type": "text"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "05eY-YwbPeju",
        "colab": {}
      },
      "source": [
        "#output_path = os.path.join(\"/content/drive/My Drive/\", \"output\", \"khattx\", \"yazeed\")\n",
        "%load_ext tensorboard\n",
        "%tensorboard --reload_interval=300 --logdir={output}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xzljv-p6c9Cp"
      },
      "source": [
        "## Run Here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gXpQX_j5cR0X",
        "outputId": "97f430dd-857d-4cbd-dcbd-fc8ebbc58551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "* source: dataset/model name (iam, rimes, khatt, khattx)\n",
        "* arch: network to be used (puigcerver, bluche, yazeed)\n",
        "* transform: transform dataset to the HDF5 file (hdf5 files are better for processing and training)\n",
        "* cv2: visualize sample from transformed dataset\n",
        "* image: predict a single image with the source parameter\n",
        "* train: train model with the source argument\n",
        "* test: evaluate and predict model with the source argument\n",
        "* epochs: number of epochs\n",
        "* batch_size: number of batches\n",
        "\"\"\"\n",
        "# NOTE: KHATTX is the cleaned version of KHATT that I created, use KHATT without an \"X\" for the vanilla KHATT.\n",
        "Main(source=\"khattx\",image=\"/content/drive/My Drive/data/demo/sec.PNG\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "####################################\n",
            "\n",
            "Prob.  - Predict\n",
            "0.0010501214 - الشاني ربها جعلت هذا البلد في وضع\n",
            "0.000777716 - الشاني ربها جعلت هذا البلد في وضيع\n",
            "0.0005211217 - الشاني ربها جعلت هذا البل في وضع\n",
            "0.00037871144 - الثاني ربها جعلت هذا البلد في وضع\n",
            "0.0002918352 - الشاني ربها جهلت هذا البلد في وضع\n",
            "0.00026788222 - الشاني ربها جعلت هذا البلة في وضع\n",
            "0.00020035134 - الشاني ربها جعلت هذا البلد في وضبع\n",
            "6.604857e-05 - الشاني ربها جعلت هذا البلد في وضح\n",
            "4.8915324e-05 - الشاني ربها جعلت هذا البلد في وضيح\n",
            "3.2776523e-05 - الشاني ربها جعلت هذا البل في وضح\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABAAAAABACAAAAAChYVogAAAgzklEQVR4nO2d13NcV3rgv3Nj5xzQ\njZwTQYKgGERSDGJS1oxnxuOSXV6v1w/7l2zV1vppX7y1tevylGs9Ox5ZswqUKJIiRYqZIAkSIHLO\naHTON52zD90NdAMNEFBDliDc31P37Zv7nO9850sHEVBRUdmrUD/2DaioqPx4qAJARWUPowoAFZU9\nDPNj38BWwZh6tbAigIpvLr69ZAj8UGf+d4IQtKvvX6VUdosAkGN+m7WYBMhvwkTGbJF9iCgzXH47\n30BObB+cJjyzm3uQkuT43P1jVR/cg+wWARB5OdVlKbKdxCf1bl2mDSvLS3y1dv0+ymioqoxbPSSd\n0mh2pqnH++kaR4kC4AdTT7ZwASUwWFFDZz7Ls8ky025pDio7xe74x4k4dFmqw/T6XxLPvug4pwMA\nABy+NdThXi8A5Pmvw+87V77iyMspW5NHuwO9Lj16ucJqL+kURIqkzKYfUAJgIaIzbvRj/PndM1WZ\ntypPXZHfNfxw96Hy0+THVPrE4GJU3tKexH/vqq/YD3js40/HxczndM+ndxJF9olc+2yUrHYx4ck/\n/be//9dxvO3bXX9XSzduJ3Qldt7w7U+GlNLvZSOIPPPNCCke7EGUyevjSqYF4NCVz6c1u2M4UNlB\nfry/nKQHbi3vP+7dyh2Q2EIQF+lpJPbi9vR8wMsiAIjf7rYXMWnhwN1+G7W6PdXzYJiNV9WW2nMB\nyML9GVLiCyShbwfsHT/gv5Aau45bNUWfFS/dvcklZQoBQKr/2nAHr9oA9hw/mgAgybuf3o3cT77r\n3EqrwxgVHcb8w4tiz1dknwYAxJmgo8gucnA2nv+d9TpHFZ2OLl3tJmJCiqdKPEl81p8u+U42Ib24\nlNgg2lMeuTPO3/W06hEQYWE2xaj9f+/xowkAJPXfGsLLr522b6XZIUoJJde3Y0LzPBnQuhs0AECU\nomq9kk5jOs96oD02FNW+38EV23d7IL27f2SmvKRuo0QjZtsPaAIgsXnGwm5wAQWowBWrW8MAYnTM\n7nZoqnw/tiEAyM42EL6+PZhoaDBuqf8QIi6G1vdvVNZZ4xOEzI0hhNgi1n2EEM3ldQG6/ILddLJq\nJ0Y7zoSHpg+zpZwCp9Im3Y6PvCQR4Uw8AAAIUUpbqO0Qkr0e03p6oH9uTgYAYE0s2QGziMpuY6sC\ngIjRtE27ky2VO8aVTZ09a97iOYkgFREAfG3XQLJuvyazCzJ71w/sJBOvs3oMt7+OMe+AAgAQmxco\nqjSxSFvtS8s7bAQkcvjhY9vpZi0AgKmiLyDkyUUiBhRXJiqCcnTUT1W2WWkASM4lkDoD2INsVQAo\nYzdn39lv3sEr047Xy0NVns3Hz6zWgXgdA8VmssjZZqTqajkAAEIIza1vxEpSxKKYdzAybugW2x5E\nEQlbxDW5HRDHSiLZWeVKWb788YjR/5sOBABYTAn5b06ZvCz9upoBAECs3aOpqtMhAILTP6ArQuWn\nyxYFgDT15b/NL773hnMHWyoy7XtFwyeCqMs0VVujO5Ys1kSNlWZNmYECAEAUxH3S2qGdCIEkLrQO\nIElYdXhtKSqQyEtRm51OBKCMX90duTuex9Ol5VPL4ZDVvkZFB4JKmo+TWPfdFD8aIQgA4oucNS8M\nUln6/PfGZqcp803j1BgNFAAgjUe3gbNQ5WfNFgWAHOgfjzxpObSj18bK5qZ4HFsUaw0AAMjoNsaK\n2vgYHa+zrIzCklhkH2qtb1CYGN5fntU8lIik079K98WR/nuLja/Rw4Pc6Y5V9QGZqzUlDt4ERElX\nYKMjUsSf4jyWEjQLytBxMmA6UoYAABDDsXkvgKQWJS6VE6WMltXrEQAAzZeoyqjsTrYoABj3vvnI\ngcM7qQCQ1FzI695sBiDPvtCWZYLTiCBq9cXulWBKq8l+JKA1r9sHaZ16is9PBSD+m9f+kz1zYRy8\nv9yw3/KKx8L+658udAG6/sjKVRpWd5aSxKjb/NBXgGiDIRbLl1rKdPdzn/XcUcv3n5HTrl/vmzId\nKEcAAHpXb2JV0GLZcNqga8mFS9Jarc2IAACEwA/qjFT5qbJVAVD+0ZsJj7u01l6I1P/7vg9+4dmk\nnWP/sF3IfKR1huK6NmOwCLlfEAix9ZGFiEU6lyPvOfHEvaGQkhm5hb5/fHHsb46/wg5POzqfks4T\naGnE2mHNExa8iU4Jmx75SnA6pTPkXZ1Er/x+TDCIzs4STHKs23KQzs5y0mFatyJlSXg+3XFIpAQ6\nk8Jk2VfNYwIAwJr4okYWlZ85hQKAECCAioTTIbbMJedNr3HJWaTSxJ8+WxQ1FzaLBFRSuXk/oiGw\nGC0SvEeIuDAv8gAAjElXrA0zZruOyt+OdPr4eNAMAECSo8MTaXnpSOWmGjAyHaUjbQ3ogwpuf74M\nRBRJFMolomBAK54BghWgN3UTEJyKFyQ5xu9dfZEk6WBp4zHSaHIfhYR5RZnAoZtfiafPJK7D6TYt\nAgBGA+EYAQDg3bZFbgf1O5VdQkH3wykBxxWjvqi7j8rT1uU4rSttzkji924HmL4/GC5uon/TSMho\nx0QM+6OhYp2CSOlgSCIIABirni4S8sJYXNTMiEW/sgE5ysWxYC0AACRmk7D0ja//jS7nZs9DO88Q\nnoXWSlygmceXRK7A6ECi837QOpwZSUVSSz7idm0Wc4wong7k1BEAUBa+7CFWYQc98lpHdOX6ONZ7\nJbFsoS7H7bUaBABCwG/KxAkyFpu+WCq1ys+cfAGAlwZm5KBY1tJqK7Zrth0piXAoOM9VVTpK8aVj\n3z18ijy5v++QcUMVgLYag1kNW06m8AZWakQwJggAWI8lviisS2hDnE6ZGu5YFQCgt8ByLNPn5JgE\nOHRnYEw8vV4QyQlBnz2MNgAA8FyBdZ6IcZlKi6t+Adnf/XASzI1HOgwAQJIv7g1BU1e7YxPZQtvK\nxpMr/Z2kJrr5067glGmn+iLSlw1E5awNALFGkyDJLOGy7lIllUplJ028CaKpHbusym4hr/cpy9c/\n7pMUbD7x4RH7BmUulHRsaej5wJKPaTr7QVXuYByXNNsLEiKxgaWTv4z+w61IfOOZJ1NbMRBRaABA\nmorGQPFAAG7FNsjVOp48eGN9XC3BRGfNe07EmvTxGKYAABSFMciCEviO1R60r+mn2Net7KsveKzC\ncyNbi33Jl8xZBXFi6tvPX6SBsQ389qCJgkTvP90MgPHge6e9G+ceU+Ym2bhioyOhce7DPytf7taY\nS1PHsShpshqbFPClMpZ+oFzHfb7zJxffKjue6eoal5XNSB/E8NEpn40v6aoqu4+8jpG++8d7QQKw\nHF8MXigrOmopyy+fPRtbCqdEtLBcbrVmtuLIg/mWTn2xAzZkuZu0t0VeexGOKBs6ApBOF4spNAAA\n7Wp6UdQ3Thnd49mPWJIJLiaGEDIVeBt4l8M3nTAhAMm34G0NDsfp8M30r886CnWRVP8nxzftEMjU\n1i6s1hpKPP3TrfEYAQgnk+iQgcx/emNKIeGob+p8i46lqGK2FQBDxdBqoA72z3Ze6NSVeZUSx2Jp\nYaIhY13hjWl/PGu4QGy7Le2x683GrLCj9Xo6K1cRkuMJNRhoz7Ha5kn8aXeIAIAyHzOUW4sZ/JWF\nK58Oh0QiSoAjw08PZLXmxP3/M362YnvptUI4FVkWMPgWNqkIgDgeZTsHRUHxmoAUQ2VjWHEsXdSA\nyVh1oZmOvOdhKo5cG5pppYEs9ym/OjD15QMbM38zqZzx5Es9Eu7vPb+5g5C1e4e4lTc4/dUXkwoA\ngDx3p77SIE59O69HaSHUvfTiQEuV08ZriuXbaJ3SYjSXh0v8swfrtcB7CxMvvkeswfLVmTPVAAB6\nrzYYXCmHYjYDgGXlqRC1GspMyI5VSlPZPawKAGFyOJg1uSXDySJmKBJ6+eBxqOZ1CxWZmFuOStPR\n7IEjn90I4Ja3XNu5rq31+ZUF8igI09LGOyGDZTmeGWEZHR0MV+d+wBjncldpPS1lK4IQIoRT63UA\n1qFfeHnaurqBKj/7bHCwiQa8sPja+03L2lhV7fS1x5i6VDB/SEVYc/FE+pUqW7SBhKP6rNiIzwdz\nby0xlyCSVFnhZf1Px9Jj00/qK91ug7PMU7nObsKZpcEJS07TIHJMIAgKJhtKzG/czIxQjMTo3cWa\nSgoA8WXWqaG6At0mPyaCiKvvHxfJtlD5mZMnAKaDkBlvKUtr7XoDnxh8dmWq6lJ1uQmiw313H0u5\nCLvIwz7WMfNxrWUbRkHkOH73xj2OczKRzdROk663PeN4tzbrRkb359xrQjDpzgSwIH3d42QCUwBA\nm7XpYFRZJwCwqCgFUgaZG41TcxgA4tSJcuRoq2w5L1BX77Ufy3fygyTxG0QqkmSYcdAAwBjkmblc\n53TsHx2NiAAAyFRlBFT+V24v6/v86mBQWlju5jRaR8f5IoZTJTk6vi8rACint6/Ru0btEIZuec54\ntyUBSGRsms/IZ6Shx4fPFamUmPkVB8M5HQvFfOL25nEqu59VAUBZm0K+UBoDZTl6rj43Z8aYZOyB\n6bnubumDE2U8S6Gy6mPW+VB7pvwG8fXaD8a+ezHQtp28drbmwmK/qevAhH6zg7TU8GIrQgCgdXCT\nk7nOTYJPoieytjdtvUVWMp4sp4kIRWKBpfmIpTBLEFEUlhSCAEupkIAjoqvC+JFwRSlw2ZP4YjAi\nFjNQkNSLF+6LOgBACAeXcnMY77v2b7pnUgQoW9dZN8XV1HMschhqPr0VVBQhAdQsOrxe2qUDorAs\nZN8CslVcvVx+rPCiiWcfG5j3tpfARPMrfyxOR6NCnl5EUli7Ik0QCgRz94QSy8IOp3yr/ORZFQC6\nTteHzy8/SxBt1384npvQk4QvXWVAQNIj3wwfbqvJ2MkoBkWUlk4HAABJTc6cuTg4+3JiuajzcB2Z\nmSYyXnTNmBqc18i6ETtX6JtgObY0MZswAQAODEcTwbA1MyQrU48qc4chRmMxZ01atLHStt6pyHrM\nBie7WhwXK5KgTU/O17DALv9vp05eWtJQpO3P59PBivyjU/7l/q66de4QIieHfjf2FhAEJOVPh325\n3quptbZ/8cmgggyn/qZdC4yRAgC2Sof9jxOAEALCrY9UBl1DzcPhUO4Hlk88fVTjZPNFUXxomhkR\nX/FWC0H2g82Q1Xq0Dc+HRgyrJpD00+iqpsPZmZwCQ2h3Tell0lR2GastknbY3IuEcIjRlZkwAgBC\niDLwnfDreorEegelo29UskAIEJIYuv2g4cMOLQAASS5FPfUxC4pvZfggBMvAIgCgyk6nOa2kFdd1\nsHiK1dMAOO2bvNvvG5zT0IgE733tVx5/+kYlB4BIuPfu4ZhCEADBy32KJxNLqwQTrn229ZZC2qi1\neDgiy4hDAASHp4d7gulnD5wmokMP05ws66/H7OAnqHB8loT4TdulGi4vmYgAJvGJnnvXUnVTtRwJ\nP+32J+63dWYiAZRUGmGZADBGQ4zjEWAAIKJ/NoIRxZs0lPFoiwbWgDTV9Y9f3NM5aAQASjoi+j5L\nHO5w5UU0YRFiC+GtZQflYiU4q0XMHqCt0/fcdNblDpdnr0WrDFkdQxa03trMLwRrapp3MtRbZVeQ\nNySRdO83I2yTMjl5X2tCBCAZEuP3v6UrDbbgswemox0aBRMhJMVDMw8elf/Z+9mgOCJHn+hHFomw\nlah4yT8b4x10Lnhfnhvzrt1FmewVatxMbHlpbHCB5rptB8ymRM+VJ2bheWxqv4kxmKPD11+k3TEb\nAlAC3Vf0FWz2/iXOUCQUUImltTaGRCYT9VZaCvufPu0PNDmHv25sSUwuRqIAEPrsiRWk4BFHgfTQ\nWsgz2fdaudeiyU4gsBgPLo4+7e5PMI8/edMu9P+pByVuGoPVOhpk4h+aXHg6hwFSL/9Qub+KBQBA\nytTNq4Oy2dbYbqKsnY3rTQBI7zYM/yHc7uYJIKH3USz+YPLpySOtq8sNmLpGn0/0ubdQspsI0ZgC\ngCgU7/Ht82bOwJUZer62ve2igSgE4fmrV+GQ04IBEE5N35puKc+JBs5i3WJmiMrPh7y/XBz4/I72\n6BvRTxc/GXEAQSQ4FU8uBbV/jNSNfRM/GXyoEIQjk3HfdIRu/OB8tv8jnYf7/LaBONLxtecmUpLO\ny50DIgQnuntC5jI6O1GnhAl969pGJ0991l9Tp52f8AVTbed7X0x7XOWB5/Pu84FHzyfLdLy7wjcy\nle6LdlfSAOJMX/BXddk7MfAJfxGfguyPEQVI5M6zg1V8bGr83qzmyC++/fih82Dou3tx4CkBB8I0\n8I0NBQIAuRqZWM/k9YaOWoddhwCARP0LI4P9PkVniw7+bswVH+zVHYo+/7eXTTYOBDz3fD4tcvWa\nQLB3RHegSQcEgJIGe0Ou6ur6Q506RPHFwgpYr33y9kBLvQkDFRt4EiXiXPDFyEfHVwwBtre1v5t7\nfHitAMBkjc9TCS/OTMyLCBAHgeFAV1NmM+Ot7u4l8XoeiCCxcv+Vl/ovFAcGQDjw5AG2ZhclIAQX\nr6mo8rNmtfspvsvX9Bfeb/bRV6YmMnGyaZmwGuG7QWPcR81dxgSAiBFMoP6D0w25FBakbTl5xd/Z\n2cusaz7ywqS1ddWgJYcnHtx+HpCYlWEQUfQvmtYOiohKDU88oMSUhPny1+Hl5BzLSwnbsY8CwqQv\nSFEcL2GHa3J0jkcAWFDqulbWtsF6exGjnZKQWRaA+G7cNrHpaDJK1Z3tij0Z/r9fp4NxZK3TLiUA\nATGfaC3on8jVbI4LvtDIHaPNY2UIILy0EIok02zFyYqHdyYCrJLWnPzL2Vjf/R4GASFiXKb0HW+7\nH96aiUajT7JSCdOtZ07Xmo0bGju1be3D/infYxoAKckkAVDi8nhgNeyRdl8YubxuAQWSSBh0BdJ1\n6d63LxeSmeUKsfH1toybBOjyk4M9z+ZMDAFZYXA4xaVuvOQIAJBkiD/XmJm/EEWi1GSgPchqZZzo\nnRvkvb9o0tkNTVduzWamw5TuWM3Ll2MECAoBIASAGEvlvlOnqvK8BxW/siknLXFubU1uEr6X6lpt\nx/LC1a97F/Jz3xFTeea9urVdlnY5FCEBAEh74O1WnxHS6Rggz/HmWPvtkExQGtiO9ywfP4tFAAAo\n7/mu7BgmzoQquoooykSyVDtZgpRIDCgMiGp87x3vqdG5pUVAtPfUW87lOCAg+vb6QmVEV3/0VkiR\nxAiwWo4igEgqjRHt2HfyksUwMxQEYGrfOmysH4omcs/j7HrvouFg7ecvUolEZov18OHO1orN5tZs\n9cmhh+nM/kAhBIgQZDbmdUbaXl1XvkZOkvTY9MH8Un+J/s9vDoezChDSd/1FY/YMyHx2aDg8CQiA\nAFCGC9X9d15mj+IOnG/ISk8p7agqqbypyq4k1+ZJsu/y/Ovvt/DA19k8+i/nMQDSVb52qXboyoM5\nDIBorclIcc762vLqWnt+V9Hut1OuWal6TbUQEn7woLNsZUd57otPnsQBrdqyaGP9xYv7jOuMgArh\nOQoI62i/dM516AN+VALaffqwjj2bejQURcA2/vJDTmPr9YkE6MpzH9Rmr0FwWUtDMSWbb2w3UQpF\n0TQAjTQ1739Qy9Qe736SRsb6N893GdMiICCMfo3Bn67964rn4z4RE0nOFBem7VZHXVtLUy11elGY\nlamyE13m2vPRHr8MABStrzh76mAFbTaYDM+iGAAYW/OJoy2bZAIAACD9vpPR0czIzZmtDEJSOGEu\nzAXQmA1rbIDyyDPIq+JJIs8+uT4p5t6u7shvjq6sN8Z691enJAAAAsh6/KP6W5NjMgAA0rR9eDQX\ndECXe7vUTIC9x4oA8N16XHa+nUeAaOtRwvYkCNLW7D/eamjyVD0LK6CzOctsDFfe7OIZujBDRl+P\nYhPzB9Z6qmeupytXjPJ49sofBqjKFT8YQZSj6sAbdUU6B1d3kWeAaGu69rvplo/KetKErT9Vz3GH\nvfsf+xFoOs80og+9j0aXY2A/dOFgzszAt7KVxRYQ1h7c18kD0jceCRBEuIrDb7axwO97mw0baw6/\nUadDLAHI6DcFIPu5ur7nAxPjMSDAl9lZq8Vd7mmu0zM0tP2Kexg17X+rkvVcNH3XFwFCma325jP1\nehq4eo2lZiROgHI0Hz5WpnlV3WC64qL1yXQKCGV0lJfzgMTJnsr8YCCSColrsq9x4P7UqVX7Ckl2\n/+HrAFeWdRWwlZcu5gVlcB0fugIEAcES3/bhcWv6OQ4oAJSl6fxbOVUOGU8YW1UNYO+Ri7VXHv6X\n3o/+LlMtFog41R/ByNrk0TOICMuj8yLYvW4jhxDFFIm2J/KTf134z8cL2g9JXfsfb/9yZdUM6cp/\nfa5rf61Ou7LCD19RaytWhgNHfQkDTYCz8jQFoMSiEtBGA4eAYCGcQsBY9DRgKbY86yNVjbaVmSsR\nRaZY6K4cpvUsAnl2wC8joqurNzEARJjqj7oaPZt3T0WOjz26PKgQ2nO8XVvhtBlomkYAQOSFp2F3\nY7kGAU4vDPgB6DKPxaKlsg7B+eEAIUxVvZ3fQuUUIidnR8JAaFe504QAyNyNuiOmvN498L/if3e0\nIJgvefdT699WrLxweewf/jld2dBRzxECAJqGxvzlFoiy1L+gIAAxbt7fqkPB7tsTIgBTd6rLunIK\nJcUUKams8nMnNyKLS/6arlwyDOJrnBIAq+MQANJ4LWkCrIbbyBNN0pNXek9XFM6glfHnhqZVX5YS\nmhU8h991MhSfnc1SmuKFuCijltCIoGzuD202YoQoBACI1mkUlJlFUDxnKheJdjWoDVBRIzsAbUMI\nABivVSQAtE6DAABpapwKp3vFmEfTnNbTuEwALDUuWrO6vghivQaZ1zEIgNJW2UUAxPMrBTVofa1L\nBECarVVNQayp0SvlnaLqXW2ezVDo/+feM/UFz0Yit+S86RVIQ4Nx21tvVupZLQUrz7hyfsZtFAgA\nYIUxahFYjjYlMQDS2/LEBK1XU4H2ItlGhJeHxXMdq82Gy3ixM19ogyHv21oIpAY/+cp1rKzwd6nv\ngd2eV0ObYBK4n/Zwhs59ubFsgxPShb0G5X/PSAUsY5ZGHLvxOQpOkN2HYwsuy3NbORppvVYRAFie\nLdydsea+I9acf14AAMTkbSJY2bwuGADK3lvuTr35dXwX/t/Vqq7CuY0cHKzNX/GTCBKkx81jyP2G\nm4H1z8WsLoWAAGizOTPpKdhP7f57kpwBbX7U9Hr+clkFzWGztkHkpcc3vhQvtBRaukloYLQyT2ul\nXG1i+N4zG28+hQ+WaGySJnpJVyW77SaLNv260UH0BhE4qOjH9VtIbGza2/iqAh+F7zs/F3Dm8pdL\n5/cVhBASORLTWfM20GWVpuDNZxq6bOpic7GEHrTmm9rbVTJkBYA0OVtZtVHKWFGwjBECBNLkjc9e\nUOfOr8kEwsElIX/YY1r+sn3enxDn56SK1tIEgDjw2dfMb94v32aG7I8EST3/Y2/Dnx8zbLnP5XIh\nAABIdPKbj2fqDroKHzbtEwsMsUzdJX48SdLLT+Y1HjWgX2XrZK1+qXFfxzqH3KbIgaUEIQjFHl4e\n1b/52661Ue6ikJqbq1kZjajyd0/HApHJPz6VS403S9z4bIAzt7pLXJXv3wmSHrg1MdbUtmnWY/7+\n4nLMaNNQAFgSU9G+29+OWs+/tkZ8xKej6fwSqbTr0uHFGJ6+8m1YLe+vsh0yAgALMdm6rRqfRF68\n2uMXFUguB0wX/urw+tV1ULr7c+e+lYx6xmJRpGBQYDuPrMuI2R4klRQhmt4lcatIX9/oK/duWQFQ\nZr542vJ2Mws4GFgYftk9FeH3X2peEy+NhVTfU7ORQgCZil6M1VyukJ6Hxta2EqsJquwtcjYAwq0N\nNdkcxJQdcy5H41gBbXVXx7psdeRoqZj+BjpdHictgEaOhsLpWMQ/af3tO+0lrsyru2gbwp0tu8Rp\njbj9//FI5dEtKwCRb/80OCAz+t6BuWDQvxTiGjsv7VsrMg01/FP5RVezg6NEQUE4Hk8kotHIXPqd\nc4dKFK8qe4uMAEAUkxwLbloafy2s234wlUhhQBqbYX1nRM4zwjdDnz10ljloAfFyLBgRweioPdfR\nUOoQxR+o80key27JXEP2Uwf1RZc1KwpttJlNRpamMBYlrqqrrqG1ZW2hBWRsbpj6tv9Ro8fCRiMy\n4EQ8EU9qHN4zbR1bXW5dRQVgRQCwutB3b1Rup+oMQhxncGIAQMUm40hz0FP+L08WKIoGggAwpuz7\nj7/eaGI3KDi+jSuzVjPZfFnRnxSUQb/1dZSQ6SL/pO6cF1zHIouLUbqm3soy6/o0W3Ep0bOweIsz\nsrEUASAEad3Hz73m5tbvq6KyCdmRiXdZl7pb1k41XwUFGzc3xHreAV1PTFGAAKfR88ZD7x808zvS\nPtHucADk2NYyapT5/AnOQAOjsVZIGLFFw6+Q+UP3Vw9nU/EIRRCiGJbzthw6X6fbRWJR5adBNhRY\nuv8/r9X+9qJXS1N5DS7fIfU9UBaG+ieDUVlkayqcel1ltWqf2jFweGa4b+b5OGWw8rqyMldDjcut\nzv5Vtk1WAJDFT/67r+HQwQaPzbLSjkgquUG47hYhQjgYTyky7bTpeZoteUFRlVUISS76eicoh1vD\nW80mm059uyrfg1wykHjn7+8kjfX1lVWtzSaZsIoI2D8OB6pKs7URRaEQKBJf8sxfZS2EKLE4GPSE\n0K8KNVZR2YBc/2YqukYmIr2DGlfHEU8SNEIS42CosaXEhiUtTleXJcfnKmuMqnVqh0EkHRQdBnHM\n56naepyhiko+OQGAPOen0z5JTETnH+gFQinAaJvfOVFRYreN3P7q/Qvj/9h37DcH1GoTO02q70/R\nS2dm/+XR4Y/aihRDVVF5NSsCQH/gb503JlOKHIkgRNMad82BE13ukqNt4lMhJTox1SxtvAiwyveD\nhG9+CeWHI5O93IWNV1hVUdmMlSk+ZTlqbno5PTEdJ7Strqa8tqq+avNaVltB33ahVVd+vun16t0S\nt7OLoDijpUbnOc41e1T/n8r3A+WNzEp8fupZnx9zVYfavE6++Gq824NEF8rMwlKkwqKaAHacZF+3\n8VQ5mVl2eLeVyKmiskK+AAAiC4tBASiL28jsUKwNVhhEFKwGqP0AEEGgtAzICq0qACrfE1Q4Nyey\nQgDRqtNORWVPgFTjnIrK3kVVzVVU9jCqAFBR2cOoAkBFZQ+jCgAVlT2MKgBUVPYwqgBQUdnD/H/c\nUjHtAAY96QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=1024x64 at 0x7FE3DE1BBA90>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "####################################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "__VHtIn3dAtx"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BN-uzb2TcmpJ",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Provides general interface to the project.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import cv2\n",
        "import h5py\n",
        "import os\n",
        "import string\n",
        "import datetime\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "arabic = True\n",
        "\n",
        "def Main(source,arch=\"yazeed\",transform=False,ocv2=False,image=\"\",train=False,test=False,epochs=1000,batch_size=16):\n",
        "\n",
        "    # Change those as needed\n",
        "    raw_path = \"/content\"\n",
        "    source_path = os.path.join(\"/content/drive/My Drive/\", \"data\", str(source)+\".hdf5\")\n",
        "    output_path = os.path.join(\"/content/drive/My Drive/\", \"output\", source, arch)\n",
        "    target_path = os.path.join(output_path, \"checkpoint_weights.hdf5\")\n",
        "    \n",
        "    \n",
        "    if(source.find(\"khatt\") != -1):\n",
        "        arabic = True\n",
        "        # Best size for KHATT & KHATTX\n",
        "        input_size = (1024, 64, 1)\n",
        "        charset_base = 'ءآأإابتةثجحخدذرزسشصضطظعغفقكلمنؤهويىئ0123456789@:,.?!\"()//\\=-_#%$^&*+ '\n",
        "    else:\n",
        "        arabic = False\n",
        "        input_size = (1024, 128, 1)\n",
        "        charset_base = string.printable[:95]\n",
        "    max_text_length = 128\n",
        "    if transform:\n",
        "        # Transforming dataset to HDF5 format\n",
        "        \n",
        "        assert os.path.exists(raw_path) # Otherwise raise exception\n",
        "        \n",
        "        print(\"The \"+str(source)+\" dataset will be transformed...\")\n",
        "\n",
        "        ds = Dataset(source=raw_path, name=source)\n",
        "        ds.read_partitions()\n",
        "        \n",
        "        print(\"Partitions will be preprocessed...\")\n",
        "        ds.preprocess_partitions(input_size=input_size)\n",
        "\n",
        "        print(\"Partitions will be saved...\")\n",
        "        os.makedirs(os.path.dirname(source_path), exist_ok=True)\n",
        "        for i in ds.partitions:\n",
        "            with h5py.File(source_path, \"a\") as hf:\n",
        "                cv2_imshow(ds.dataset[i]['dt'][0])\n",
        "                hf.create_dataset(str(i)+\"/dt\", data=ds.dataset[i]['dt'], compression=\"gzip\", compression_opts=9)\n",
        "                hf.create_dataset(str(i)+\"/gt\", data=ds.dataset[i]['gt'], compression=\"gzip\", compression_opts=9)\n",
        "                print(\"[OK] \"+str(i)+\" partition.\")\n",
        "        print(\"Transformation finished.\")\n",
        "\n",
        "    elif ocv2:\n",
        "        # Visualising the prediction vs original label for a subset of the testing set\n",
        "        with h5py.File(source_path, \"r\") as hf:\n",
        "            dt = hf['test']['dt'][:256]\n",
        "            gt = hf['test']['gt'][:256]\n",
        "\n",
        "        predict_file = os.path.join(output_path, \"predict.txt\")\n",
        "        predicts = [''] * len(dt)\n",
        "\n",
        "        if os.path.isfile(predict_file):\n",
        "            with open(predict_file, \"r\") as lg:\n",
        "                predicts = [line[4:] for line in lg if line.startswith(\"TE_P\")]\n",
        "\n",
        "        for x in range(len(dt)):\n",
        "            cv2_imshow( adjust_to_see(dt[x]))\n",
        "            print(\"Image shape:\\t\"+str(dt[x].shape))\n",
        "            print(\"Ground truth:\\t\"+str(gt[x].decode()))\n",
        "            print(\"Predict:\\t\"+str(predicts[x])+\"\\n\")\n",
        "\n",
        "\n",
        "    elif image:\n",
        "        # Testing for a single image\n",
        "        tokenizer = Tokenizer(chars=charset_base, max_text_length=max_text_length)\n",
        "\n",
        "        img = preprocess(image, input_size=input_size)\n",
        "        x_test = normalization([img])\n",
        "\n",
        "        model = NNModel(architecture=arch,\n",
        "                         input_size=input_size,\n",
        "                         vocab_size=tokenizer.vocab_size,\n",
        "                         top_paths=10)\n",
        "\n",
        "        model.compile()\n",
        "        model.load_checkpoint(target=target_path)\n",
        "\n",
        "        predicts, probabilities = model.predict(x_test, ctc_decode=True)\n",
        "        predicts = [[tokenizer.decode(x) for x in y] for y in predicts]\n",
        "\n",
        "        print(\"\\n####################################\")\n",
        "        for i, (pred, prob) in enumerate(zip(predicts, probabilities)):\n",
        "            print(\"\\nProb.  - Predict\")\n",
        "\n",
        "            for (pd, pb) in zip(pred, prob):\n",
        "                print(str(pb)+\" - \"+str(pd))\n",
        "\n",
        "            cv2_imshow(adjust_to_see(img))\n",
        "        print(\"\\n####################################\")\n",
        "\n",
        "    else:\n",
        "        # Either training or testing\n",
        "        assert os.path.isfile(source_path) or os.path.isfile(target_path)\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        dtgen = DataGenerator(source=source_path,\n",
        "                              batch_size=batch_size,\n",
        "                              charset=charset_base,\n",
        "                              max_text_length=max_text_length,\n",
        "                              predict=test)\n",
        "        # Creating a model\n",
        "        model = NNModel(architecture=arch,\n",
        "                         input_size=input_size,\n",
        "                         vocab_size=dtgen.tokenizer.vocab_size)\n",
        "\n",
        "        # set `learning_rate` parameter or get architecture default value\n",
        "        model.compile(learning_rate=0.001)\n",
        "        model.load_checkpoint(target=target_path) # Very useful with the colab issue we talked about yesterday\n",
        "        \n",
        "        if train:\n",
        "            print(f\"Train images: {dtgen.size['train']}\")\n",
        "            print(f\"Validation images: {dtgen.size['valid']}\")\n",
        "            print(f\"Test images: {dtgen.size['test']}\")\n",
        "            model.summary(output_path, \"summary.txt\")\n",
        "            callbacks = model.get_callbacks(logdir=output_path, checkpoint=target_path, verbose=1)\n",
        "            start_time = datetime.datetime.now()\n",
        "            h = model.fit(x=dtgen.next_train_batch(),\n",
        "                          epochs=epochs,\n",
        "                          steps_per_epoch=dtgen.steps['train'],\n",
        "                          validation_data=dtgen.next_valid_batch(),\n",
        "                          validation_steps=dtgen.steps['valid'],\n",
        "                          callbacks=callbacks,\n",
        "                          shuffle=True,\n",
        "                          verbose=1)\n",
        "\n",
        "            total_time = datetime.datetime.now() - start_time\n",
        "\n",
        "            loss = h.history['loss']\n",
        "            val_loss = h.history['val_loss']\n",
        "\n",
        "            min_val_loss = min(val_loss)\n",
        "            min_val_loss_i = val_loss.index(min_val_loss)\n",
        "\n",
        "            time_epoch = (total_time / len(loss))\n",
        "            total_item = (dtgen.size['train'] + dtgen.size['valid'])\n",
        "            t_corpus = \"\"\n",
        "            t_corpus = \"\\n\".join([\n",
        "                f\"Total train images:      {dtgen.size['train']}\",\n",
        "                f\"Total validation images: {dtgen.size['valid']}\",\n",
        "                f\"Batch:                   {dtgen.batch_size}\\n\",\n",
        "                f\"Total time:              {total_time}\",\n",
        "                f\"Time per epoch:          {time_epoch}\",\n",
        "                f\"Time per item:           {time_epoch / total_item}\\n\",\n",
        "                f\"Total epochs:            {len(loss)}\",\n",
        "                f\"Best epoch               {min_val_loss_i + 1}\\n\",\n",
        "                f\"Training loss:           {loss[min_val_loss_i]:.8f}\",\n",
        "                f\"Validation loss:         {min_val_loss:.8f}\"\n",
        "            ])\n",
        "\n",
        "            with open(os.path.join(output_path, \"train.txt\"), \"w\") as lg:\n",
        "                lg.write(t_corpus)\n",
        "                print(t_corpus)\n",
        "\n",
        "        elif test:\n",
        "            print(f\"Test images: {dtgen.size['test']}\")\n",
        "            start_time = datetime.datetime.now()\n",
        "\n",
        "            predicts, _ = model.predict(x=dtgen.next_test_batch(),\n",
        "                                        steps=dtgen.steps['test'],\n",
        "                                        ctc_decode=True,\n",
        "                                        verbose=1)\n",
        "\n",
        "            predicts = [dtgen.tokenizer.decode(x[0]) for x in predicts]\n",
        "\n",
        "            total_time = datetime.datetime.now() - start_time\n",
        "\n",
        "            with open(os.path.join(output_path, \"predict.txt\"), \"w\") as lg:\n",
        "                for pd, gt in zip(predicts, dtgen.dataset['test']['gt']):\n",
        "                    lg.write(\"TE_L \"+str(gt)+\"\\nTE_P\" +str(pd)+\"\\n\")\n",
        "\n",
        "            evaluate = ocr_metrics(predicts=predicts,\n",
        "                                              ground_truth=dtgen.dataset['test']['gt'],\n",
        "                                              norm_accentuation=False,\n",
        "                                              norm_punctuation=False)\n",
        "\n",
        "            e_corpus = \"\"\n",
        "            e_corpus = \"\\n\".join([\n",
        "                f\"Total test images:    {dtgen.size['test']}\",\n",
        "                f\"Total time:           {total_time}\",\n",
        "                f\"Time per item:        {total_time / dtgen.size['test']}\\n\",\n",
        "                f\"Metrics:\",\n",
        "                f\"Character Error Rate: {evaluate[0]:.8f}\",\n",
        "                f\"Word Error Rate:      {evaluate[1]:.8f}\",\n",
        "                f\"Sequence Error Rate:  {evaluate[2]:.8f}\"\n",
        "            ])\n",
        "\n",
        "            with open(os.path.join(output_path, \"evaluate.txt\"), \"w\") as lg:\n",
        "                lg.write(e_corpus)\n",
        "                print(e_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vP2VpgVIdDTF"
      },
      "source": [
        "## Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5EkyP6-XcqN7",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Gated implementations\n",
        "    GatedConv2D: Introduce a Conv2D layer (same number of filters) to multiply with its sigmoid activation.\n",
        "    FullGatedConv2D: Introduce a Conv2D to extract features (linear and sigmoid), making a full gated process.\n",
        "                     This process will double number of filters to make one convolutional process.\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer, Conv2D, Multiply, Activation\n",
        "\n",
        "\n",
        "class GatedConv2D(Conv2D):\n",
        "    \"\"\"Gated Convolutional Class\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(GatedConv2D, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Apply gated convolution\"\"\"\n",
        "\n",
        "        output = super(GatedConv2D, self).call(inputs)\n",
        "        linear = Activation(\"linear\")(inputs)\n",
        "        sigmoid = Activation(\"sigmoid\")(output)\n",
        "\n",
        "        return Multiply()([linear, sigmoid])\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Return the config of the layer\"\"\"\n",
        "\n",
        "        config = super(GatedConv2D, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Tensorflow Keras layer implementation of the gated convolution.\n",
        "    Args:\n",
        "        filters (int): Number of output filters.\n",
        "        kwargs: Other Conv2D keyword arguments.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class FullGatedConv2D(Conv2D):\n",
        "    \"\"\"Gated Convolutional Class\"\"\"\n",
        "\n",
        "    def __init__(self, filters, **kwargs):\n",
        "        super(FullGatedConv2D, self).__init__(filters=filters * 2, **kwargs)\n",
        "        self.nb_filters = filters\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Apply gated convolution\"\"\"\n",
        "\n",
        "        output = super(FullGatedConv2D, self).call(inputs)\n",
        "        linear = Activation(\"linear\")(output[:, :, :, :self.nb_filters])\n",
        "        sigmoid = Activation(\"sigmoid\")(output[:, :, :, self.nb_filters:])\n",
        "\n",
        "        return Multiply()([linear, sigmoid])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"Compute shape of layer output\"\"\"\n",
        "\n",
        "        output_shape = super(FullGatedConv2D, self).compute_output_shape(input_shape)\n",
        "        return tuple(output_shape[:3]) + (self.nb_filters,)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Return the config of the layer\"\"\"\n",
        "\n",
        "        config = super(FullGatedConv2D, self).get_config()\n",
        "        config['nb_filters'] = self.nb_filters\n",
        "        del config['filters']\n",
        "        return config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nVNA5hEzdFy6"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pjjk_cPmcsBI",
        "colab": {}
      },
      "source": [
        "\"\"\"Handwritten Text Recognition Neural Network\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from contextlib import redirect_stdout\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from tensorflow.keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, Bidirectional, LSTM, GRU, Dense\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, PReLU\n",
        "from tensorflow.keras.layers import Input, Add, Activation, Lambda, MaxPooling2D, Reshape\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "NNModel Class\n",
        "The NNModel class use Tensorflow 2 Keras module for the use of the\n",
        "Connectionist Temporal Classification (CTC) with the Hadwritten Text Recognition.\n",
        "\n",
        "x is the input features and y the labels.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class NNModel:\n",
        "\n",
        "    def __init__(self,\n",
        "                 architecture,\n",
        "                 input_size,\n",
        "                 vocab_size,\n",
        "                 greedy=False,\n",
        "                 beam_width=10,\n",
        "                 top_paths=1):\n",
        "        \"\"\"\n",
        "        Initialization of a NN Model.\n",
        "\n",
        "        parameters:\n",
        "            architecture: option of the architecture model to build and compile\n",
        "            greedy, beam_width, top_paths: Parameters of the CTC decoding\n",
        "        \"\"\"\n",
        "\n",
        "        self.architecture = globals()[architecture]\n",
        "        self.input_size = input_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.model = None\n",
        "        self.greedy = greedy\n",
        "        self.beam_width = beam_width\n",
        "        self.top_paths = max(1, top_paths)\n",
        "    def summary(self, output=None, target=None):\n",
        "        \"\"\"Show/Save model structure (summary)\"\"\"\n",
        "\n",
        "        self.model.summary()\n",
        "\n",
        "        if target is not None:\n",
        "            os.makedirs(output, exist_ok=True)\n",
        "\n",
        "            with open(os.path.join(output, target), \"w\") as f:\n",
        "                with redirect_stdout(f):\n",
        "                    self.model.summary()\n",
        "\n",
        "    def load_checkpoint(self, target):\n",
        "        \"\"\" Load a model with checkpoint file\"\"\"\n",
        "\n",
        "        if os.path.isfile(target):\n",
        "            if self.model is None:\n",
        "                self.compile()\n",
        "\n",
        "            self.model.load_weights(target)\n",
        "\n",
        "    def get_callbacks(self, logdir, checkpoint, monitor=\"val_loss\", verbose=0):\n",
        "        \"\"\"Setup the list of callbacks for the model\"\"\"\n",
        "\n",
        "        callbacks = [\n",
        "            CSVLogger(\n",
        "                filename=os.path.join(logdir, \"epochs.log\"),\n",
        "                separator=\";\",\n",
        "                append=True),\n",
        "            TensorBoard(\n",
        "                log_dir=logdir,\n",
        "                histogram_freq=10,\n",
        "                profile_batch=0,\n",
        "                write_graph=True,\n",
        "                write_images=False,\n",
        "                update_freq=\"epoch\"),\n",
        "            ModelCheckpoint(\n",
        "                filepath=checkpoint,\n",
        "                monitor=monitor,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                verbose=verbose),\n",
        "            EarlyStopping(\n",
        "                monitor=monitor,\n",
        "                min_delta=1e-8,\n",
        "                patience=20, # Change this if the training keeps stopping, happened to me earlier.\n",
        "                restore_best_weights=True,\n",
        "                verbose=verbose),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor=monitor,\n",
        "                min_delta=1e-8,\n",
        "                factor=0.2,\n",
        "                patience=15,\n",
        "                verbose=verbose)\n",
        "        ]\n",
        "\n",
        "        return callbacks\n",
        "\n",
        "    def compile(self, learning_rate=None):\n",
        "        \"\"\"\n",
        "        Configures the NN Model for training/predict.\n",
        "\n",
        "        optimizer: optimizer for training\n",
        "        \"\"\"\n",
        "\n",
        "        # define inputs, outputs and optimizer of the chosen architecture\n",
        "        outs = self.architecture(self.input_size, self.vocab_size + 1, learning_rate)\n",
        "        inputs, outputs, optimizer = outs\n",
        "\n",
        "        # create and compile\n",
        "        self.model = Model(inputs=inputs, outputs=outputs)\n",
        "        self.model.compile(optimizer=optimizer, loss=self.ctc_loss_lambda_func)\n",
        "\n",
        "    def fit(self,\n",
        "            x=None,\n",
        "            y=None,\n",
        "            batch_size=None,\n",
        "            epochs=1,\n",
        "            verbose=1,\n",
        "            callbacks=None,\n",
        "            validation_split=0.0,\n",
        "            validation_data=None,\n",
        "            shuffle=True,\n",
        "            class_weight=None,\n",
        "            sample_weight=None,\n",
        "            initial_epoch=0,\n",
        "            steps_per_epoch=None,\n",
        "            validation_steps=None,\n",
        "            validation_freq=1,\n",
        "            max_queue_size=10,\n",
        "            workers=1,\n",
        "            use_multiprocessing=False,\n",
        "            **kwargs):\n",
        "        \"\"\"\n",
        "        Model training on data yielded (fit function has support to generator).\n",
        "        A fit() abstration function of TensorFlow 2.\n",
        "\n",
        "        Provide x parameter of the form: yielding (x, y, sample_weight).\n",
        "\n",
        "        return: A history object\n",
        "        \"\"\"\n",
        "\n",
        "        out = self.model.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
        "                             callbacks=callbacks, validation_split=validation_split,\n",
        "                             validation_data=validation_data, shuffle=shuffle,\n",
        "                             class_weight=class_weight, sample_weight=sample_weight,\n",
        "                             initial_epoch=initial_epoch, steps_per_epoch=steps_per_epoch,\n",
        "                             validation_steps=validation_steps, validation_freq=validation_freq,\n",
        "                             max_queue_size=max_queue_size, workers=workers,\n",
        "                             use_multiprocessing=use_multiprocessing, **kwargs)\n",
        "        return out\n",
        "\n",
        "    def predict(self,\n",
        "                x,\n",
        "                batch_size=None,\n",
        "                verbose=0,\n",
        "                steps=1,\n",
        "                callbacks=None,\n",
        "                max_queue_size=10,\n",
        "                workers=1,\n",
        "                use_multiprocessing=False,\n",
        "                ctc_decode=True):\n",
        "        \"\"\"\n",
        "        Model predicting on data yielded (predict function has support to generator).\n",
        "        A predict() abstration function of TensorFlow 2.\n",
        "\n",
        "        Provide x parameter of the form: yielding [x].\n",
        "\n",
        "        returns: raw data on `ctc_decode=False` or CTC decode on `ctc_decode=True` (both with probabilities)\n",
        "        \"\"\"\n",
        "\n",
        "        self.model._make_predict_function()\n",
        "\n",
        "        if verbose == 1:\n",
        "            print(\"Model Predict\")\n",
        "\n",
        "        out = self.model.predict(x=x, batch_size=batch_size, verbose=verbose, steps=steps,\n",
        "                                 callbacks=callbacks, max_queue_size=max_queue_size,\n",
        "                                 workers=workers, use_multiprocessing=use_multiprocessing)\n",
        "\n",
        "        if not ctc_decode:\n",
        "            return np.log(out.clip(min=1e-8))\n",
        "\n",
        "        steps_done = 0\n",
        "        if verbose == 1:\n",
        "            print(\"CTC Decode\")\n",
        "            progbar = tf.keras.utils.Progbar(target=steps)\n",
        "\n",
        "        batch_size = int(np.ceil(len(out) / steps))\n",
        "        input_length = len(max(out, key=len))\n",
        "\n",
        "        predicts, probabilities = [], []\n",
        "\n",
        "        while steps_done < steps:\n",
        "            index = steps_done * batch_size\n",
        "            until = index + batch_size\n",
        "\n",
        "            x_test = np.asarray(out[index:until])\n",
        "            x_test_len = np.asarray([input_length for _ in range(len(x_test))])\n",
        "\n",
        "            decode, log = K.ctc_decode(x_test,\n",
        "                                       x_test_len,\n",
        "                                       greedy=self.greedy,\n",
        "                                       beam_width=self.beam_width,\n",
        "                                       top_paths=self.top_paths)\n",
        "\n",
        "            probabilities.extend([np.exp(x) for x in log])\n",
        "            decode = [[[int(p) for p in x if p != -1] for x in y] for y in decode]\n",
        "            predicts.extend(np.swapaxes(decode, 0, 1))\n",
        "\n",
        "            steps_done += 1\n",
        "            if verbose == 1:\n",
        "                progbar.update(steps_done)\n",
        "\n",
        "        return (predicts, probabilities)\n",
        "\n",
        "    @staticmethod\n",
        "    def ctc_loss_lambda_func(y_true, y_pred):\n",
        "        \"\"\"Function for computing the CTC loss\"\"\"\n",
        "\n",
        "        if len(y_true.shape) > 2:\n",
        "            y_true = tf.squeeze(y_true)\n",
        "\n",
        "        # y_pred.shape = (batch_size, string_length, alphabet_size_1_hot_encoded)\n",
        "        # output of every model is softmax\n",
        "        # so sum across alphabet_size_1_hot_encoded give 1\n",
        "        #               string_length give string length\n",
        "        input_length = tf.math.reduce_sum(y_pred, axis=-1, keepdims=False)\n",
        "        input_length = tf.math.reduce_sum(input_length, axis=-1, keepdims=True)\n",
        "\n",
        "        # y_true strings are padded with 0\n",
        "        # so sum of non-zero gives number of characters in this string\n",
        "        label_length = tf.math.count_nonzero(y_true, axis=-1, keepdims=True, dtype=\"int64\")\n",
        "\n",
        "        loss = K.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "\n",
        "        # average loss across all entries in the batch\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Networks to the Handwritten Text Recognition Model\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def bluche(input_size, d_model, learning_rate):\n",
        "    \"\"\"\n",
        "    Gated Convolucional Recurrent Neural Network by Bluche et al.\n",
        "\n",
        "    Reference:\n",
        "        Bluche, T., Messina, R.:\n",
        "        Gated convolutional recurrent neural networks for multilingual handwriting recognition.\n",
        "    \"\"\"\n",
        "\n",
        "    input_data = Input(name=\"input\", shape=input_size)\n",
        "    cnn = Reshape((input_size[0] // 2, input_size[1] // 2, input_size[2] * 4))(input_data)\n",
        "\n",
        "    cnn = Conv2D(filters=8, kernel_size=(3,3), strides=(1,1), padding=\"same\", activation=\"tanh\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=16, kernel_size=(2,4), strides=(2,4), padding=\"same\", activation=\"tanh\")(cnn)\n",
        "    cnn = GatedConv2D(filters=16, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\", activation=\"tanh\")(cnn)\n",
        "    cnn = GatedConv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=64, kernel_size=(2,4), strides=(2,4), padding=\"same\", activation=\"tanh\")(cnn)\n",
        "    cnn = GatedConv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding=\"same\", activation=\"tanh\")(cnn)\n",
        "    cnn = MaxPooling2D(pool_size=(1,4), strides=(1,4), padding=\"valid\")(cnn)\n",
        "\n",
        "    shape = cnn.get_shape()\n",
        "    blstm = Reshape((shape[1], shape[2] * shape[3]))(cnn)\n",
        "\n",
        "    blstm = Bidirectional(LSTM(units=128, return_sequences=True))(blstm)\n",
        "    blstm = Dense(units=128, activation=\"tanh\")(blstm)\n",
        "\n",
        "    blstm = Bidirectional(LSTM(units=128, return_sequences=True))(blstm)\n",
        "    output_data = Dense(units=d_model, activation=\"softmax\")(blstm)\n",
        "\n",
        "    if learning_rate is None:\n",
        "        learning_rate = 4e-4\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    return (input_data, output_data, optimizer)\n",
        "\n",
        "\n",
        "def puigcerver(input_size, d_model, learning_rate):\n",
        "    \"\"\"\n",
        "    Convolutional Recurrent Neural Network by Puigcerver et al.\n",
        "\n",
        "    Reference:\n",
        "        Joan Puigcerver.\n",
        "        Are multidimensional recurrent layers really necessary for handwritten text recognition?\n",
        "    \"\"\"\n",
        "\n",
        "    input_data = Input(name=\"input\", shape=input_size)\n",
        "\n",
        "    cnn = Conv2D(filters=16, kernel_size=(3,3), strides=(1,1), padding=\"same\")(input_data)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
        "    cnn = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
        "    cnn = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\")(cnn)\n",
        "\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "    cnn = Conv2D(filters=48, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
        "    cnn = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\")(cnn)\n",
        "\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "    cnn = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
        "\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "    cnn = Conv2D(filters=80, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
        "\n",
        "    shape = cnn.get_shape()\n",
        "    blstm = Reshape((shape[1], shape[2] * shape[3]))(cnn)\n",
        "\n",
        "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
        "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
        "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
        "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
        "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
        "\n",
        "    blstm = Dropout(rate=0.5)(blstm)\n",
        "    output_data = Dense(units=d_model, activation=\"softmax\")(blstm)\n",
        "\n",
        "    if learning_rate is None:\n",
        "        learning_rate = 3e-4\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    return (input_data, output_data, optimizer)\n",
        "\n",
        "def yazeed(input_size, d_model, learning_rate):\n",
        "    \"\"\"\n",
        "    Gated Convolutional Recurrent Neural Network by Yazeed Mohi-Eldeen Sabil.\n",
        "    Based on the Flor et al architecture, modified some portions to fit the KHATT dataset. \n",
        "    \"\"\"\n",
        "\n",
        "    input_data = Input(name=\"input\", shape=input_size)\n",
        "\n",
        "    cnn = Conv2D(filters=16, kernel_size=(3,3), strides=(2,2), padding=\"same\", kernel_initializer=\"he_uniform\")(input_data)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "    cnn = FullGatedConv2D(filters=16, kernel_size=(3,3), padding=\"same\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "    cnn = FullGatedConv2D(filters=32, kernel_size=(3,3), padding=\"same\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=40, kernel_size=(2,4), strides=(2,4), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "    cnn = FullGatedConv2D(filters=40, kernel_size=(3,3), padding=\"same\", kernel_constraint=MaxNorm(4, [0,1,2]))(cnn)\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=48, kernel_size=(3,3), strides=(1,1), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "    cnn = FullGatedConv2D(filters=48, kernel_size=(3,3), padding=\"same\", kernel_constraint=MaxNorm(4, [0,1,2]))(cnn)\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=56, kernel_size=(2,4), strides=(2,4), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "    cnn = FullGatedConv2D(filters=56, kernel_size=(3,3), padding=\"same\", kernel_constraint=MaxNorm(4, [0,1,2]))(cnn)\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "\n",
        "    cnn = MaxPooling2D(pool_size=(1,2), strides=(1,2), padding=\"valid\")(cnn)\n",
        "\n",
        "    shape = cnn.get_shape()\n",
        "    nb_units = shape[2] * shape[3]\n",
        "\n",
        "    bgru = Reshape((shape[1], nb_units))(cnn)\n",
        "\n",
        "    bgru = Bidirectional(GRU(units=nb_units, return_sequences=True, dropout=0.5))(bgru)\n",
        "    bgru = Dense(units=nb_units * 2)(bgru)\n",
        "\n",
        "    bgru = Bidirectional(GRU(units=nb_units, return_sequences=True, dropout=0.5))(bgru)\n",
        "    output_data = Dense(units=d_model, activation=\"softmax\")(bgru)\n",
        "\n",
        "    if learning_rate is None:\n",
        "        learning_rate = 5e-4\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    return (input_data, output_data, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iHvsP0PSdJbT"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C1ZTDTuUcva6",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Tool to metrics calculation through data and label (string and string).\n",
        "Calculation from Optical Character Recognition (OCR) metrics with editdistance.\n",
        "\"\"\"\n",
        "\n",
        "import string\n",
        "import unicodedata\n",
        "import editdistance\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def ocr_metrics(predicts, ground_truth, norm_accentuation=False, norm_punctuation=False):\n",
        "    \"\"\"Calculate Character Error Rate (CER), Word Error Rate (WER) and Sequence Error Rate (SER)\"\"\"\n",
        "\n",
        "    if len(predicts) == 0 or len(ground_truth) == 0:\n",
        "        return (1, 1, 1)\n",
        "\n",
        "    cer, wer, ser = [], [], []\n",
        "\n",
        "    for (pd, gt) in zip(predicts, ground_truth):\n",
        "        pd, gt = pd.lower(), gt.lower()\n",
        "\n",
        "        if norm_accentuation:\n",
        "            pd = unicodedata.normalize(\"NFKD\", pd).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
        "            gt = unicodedata.normalize(\"NFKD\", gt).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
        "\n",
        "        if norm_punctuation:\n",
        "            pd = pd.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "            gt = gt.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "        pd_cer, gt_cer = list(pd), list(gt)\n",
        "        dist = editdistance.eval(pd_cer, gt_cer)\n",
        "        cer.append(dist / (max(len(pd_cer), len(gt_cer))))\n",
        "\n",
        "        pd_wer, gt_wer = pd.split(), gt.split()\n",
        "        dist = editdistance.eval(pd_wer, gt_wer)\n",
        "        wer.append(dist / (max(len(pd_wer), len(gt_wer))))\n",
        "\n",
        "        pd_ser, gt_ser = [pd], [gt]\n",
        "        dist = editdistance.eval(pd_ser, gt_ser)\n",
        "        ser.append(dist / (max(len(pd_ser), len(gt_ser))))\n",
        "\n",
        "    metrics = [cer, wer, ser]\n",
        "    metrics = np.mean(metrics, axis=1)\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HWGSln0kdNvo"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3LoB6R3ncx_L",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Uses generator functions to supply train/test with data.\n",
        "Creates and supplies data as needed.\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "class DataGenerator():\n",
        "    \"\"\"Generator class\"\"\"\n",
        "\n",
        "    def __init__(self, source, batch_size, charset, max_text_length, predict=False):\n",
        "        self.tokenizer = Tokenizer(charset, max_text_length)\n",
        "        self.batch_size = batch_size\n",
        "        self.partitions = ['test'] if predict else ['train', 'valid', 'test']\n",
        "\n",
        "        self.size = dict()\n",
        "        self.steps = dict()\n",
        "        self.index = dict()\n",
        "        self.dataset = dict()\n",
        "\n",
        "        with h5py.File(source, \"r\") as f:\n",
        "            for pt in self.partitions:\n",
        "                self.dataset[pt] = dict()\n",
        "                self.dataset[pt]['dt'] = f[pt]['dt'][:]\n",
        "                self.dataset[pt]['gt'] = f[pt]['gt'][:]\n",
        "        for pt in self.partitions:\n",
        "            # decode sentences from byte\n",
        "            self.dataset[pt]['gt'] = [x.decode() for x in self.dataset[pt]['gt']]\n",
        "\n",
        "            # set size and setps\n",
        "            self.size[pt] = len(self.dataset[pt]['gt'])\n",
        "            self.steps[pt] = int(np.ceil(self.size[pt] / self.batch_size))\n",
        "            self.index[pt] = 0\n",
        "    def next_train_batch(self):\n",
        "        \"\"\"Get the next batch from train partition (yield)\"\"\"\n",
        "        while True:\n",
        "            if self.index['train'] >= self.size['train']:\n",
        "                self.index['train'] = 0\n",
        "\n",
        "            index = self.index['train']\n",
        "            until = self.index['train'] + self.batch_size\n",
        "            self.index['train'] = until\n",
        "\n",
        "            x_train = self.dataset['train']['dt'][index:until]\n",
        "            y_train = self.dataset['train']['gt'][index:until]\n",
        "\n",
        "            x_train = augmentation(x_train,\n",
        "                                      rotation_range=1.5,\n",
        "                                      scale_range=0.05,\n",
        "                                      height_shift_range=0.025,\n",
        "                                      width_shift_range=0.05,\n",
        "                                      erode_range=5,\n",
        "                                      dilate_range=3)\n",
        "\n",
        "            x_train = normalization(x_train)\n",
        "            y_train = [self.tokenizer.encode(y) for y in y_train]\n",
        "            y_train = pad_sequences(y_train, maxlen=self.tokenizer.maxlen, padding=\"post\")\n",
        "            #print(len(y_train[0]))\n",
        "            yield (x_train, y_train,[])\n",
        "\n",
        "    def next_valid_batch(self):\n",
        "        \"\"\"Get the next batch from validation partition (yield)\"\"\"\n",
        "\n",
        "        while True:\n",
        "            if self.index['valid'] >= self.size['valid']:\n",
        "                self.index['valid'] = 0\n",
        "\n",
        "            index = self.index['valid']\n",
        "            until = self.index['valid'] + self.batch_size\n",
        "            self.index['valid'] = until\n",
        "\n",
        "            x_valid = self.dataset['valid']['dt'][index:until]\n",
        "            y_valid = self.dataset['valid']['gt'][index:until]\n",
        "\n",
        "            x_valid = normalization(x_valid)\n",
        "\n",
        "            y_valid = [self.tokenizer.encode(y) for y in y_valid]\n",
        "            y_valid = pad_sequences(y_valid, maxlen=self.tokenizer.maxlen, padding=\"post\")\n",
        "\n",
        "            yield (x_valid, y_valid, [])\n",
        "\n",
        "    def next_test_batch(self):\n",
        "        \"\"\"Return model predict parameters\"\"\"\n",
        "\n",
        "        while True:\n",
        "            if self.index['test'] >= self.size['test']:\n",
        "                self.index['test'] = 0\n",
        "                break\n",
        "\n",
        "            index = self.index['test']\n",
        "            until = self.index['test'] + self.batch_size\n",
        "            self.index['test'] = until\n",
        "\n",
        "            x_test = self.dataset['test']['dt'][index:until]\n",
        "            x_test = normalization(x_test)\n",
        "\n",
        "            yield x_test\n",
        "\n",
        "# Found this part online, modified the encoding part to fit the Arabic text.\n",
        "class Tokenizer():\n",
        "    \"\"\"Manages tokens functions and charset/dictionary properties\"\"\"\n",
        "\n",
        "    def __init__(self, chars, max_text_length=128):\n",
        "        self.PAD_TK, self.UNK_TK = \"¶\", \"¤\" # PAD = Blank white space before and after text, UNK = Unknown character\n",
        "        self.chars = (self.PAD_TK + self.UNK_TK + chars)\n",
        "\n",
        "        self.PAD = self.chars.find(self.PAD_TK)\n",
        "        self.UNK = self.chars.find(self.UNK_TK)\n",
        "\n",
        "        self.vocab_size = len(self.chars)\n",
        "        self.maxlen = max_text_length\n",
        "    def encode(self, text):\n",
        "        \"\"\"Encode text to vector\"\"\"\n",
        "\n",
        "        #text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
        "        text = \" \".join(text.split())\n",
        "        encoded = []\n",
        "        for item in text:\n",
        "            index = self.chars.find(item)\n",
        "            index = self.UNK if index == -1 else index\n",
        "            encoded.append(index)\n",
        "        \n",
        "        return np.asarray(encoded)\n",
        "\n",
        "    def decode(self, text):\n",
        "        \"\"\"Decode vector to text\"\"\"\n",
        "\n",
        "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
        "        decoded = self.remove_tokens(decoded)\n",
        "        decoded = text_standardize(decoded)\n",
        "\n",
        "        return decoded\n",
        "\n",
        "    def remove_tokens(self, text):\n",
        "        \"\"\"Remove tokens (PAD) from text\"\"\"\n",
        "\n",
        "        return text.replace(self.PAD_TK, \"\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fe2RQx4pdQ9T"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ENxLQDoJcz8q",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Data preproc functions:\n",
        "    adjust_to_see: adjust image to better visualize (rotate and transpose)\n",
        "    augmentation: apply variations to a list of images\n",
        "    normalization: apply normalization and variations on images (if required)\n",
        "    preprocess: main function for preprocess.\n",
        "        Make the image:\n",
        "            illumination_compensation: apply illumination regularitation\n",
        "            remove_cursive_style: remove cursive style from image (if necessary)\n",
        "            sauvola: apply sauvola binarization\n",
        "    text_standardize: preprocess and standardize sentence\n",
        "    generate_multigrams: generate n-grams of the sentence\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import cv2\n",
        "import html\n",
        "import string\n",
        "import numpy as np\n",
        "#import numba as nb\n",
        "\n",
        "arabic = True\n",
        "def adjust_to_see(img):\n",
        "    \"\"\"Rotate and transpose to image visualize (cv2 method or jupyter notebook)\"\"\"\n",
        "\n",
        "    (h, w) = img.shape[:2]\n",
        "    (cX, cY) = (w // 2, h // 2)\n",
        "\n",
        "    M = cv2.getRotationMatrix2D((cX, cY), -90, 1.0)\n",
        "    cos = np.abs(M[0, 0])\n",
        "    sin = np.abs(M[0, 1])\n",
        "\n",
        "    nW = int((h * sin) + (w * cos))\n",
        "    nH = int((h * cos) + (w * sin))\n",
        "\n",
        "    M[0, 2] += (nW / 2) - cX\n",
        "    M[1, 2] += (nH / 2) - cY\n",
        "\n",
        "    img = cv2.warpAffine(img, M, (nW + 1, nH + 1))\n",
        "    img = cv2.warpAffine(img.transpose(), M, (nW, nH))\n",
        "    if arabic:\n",
        "      img = cv2.flip(img,1)\n",
        "    return img\n",
        "\n",
        "\n",
        "def augmentation(imgs,\n",
        "                 rotation_range=0,\n",
        "                 scale_range=0,\n",
        "                 height_shift_range=0,\n",
        "                 width_shift_range=0,\n",
        "                 dilate_range=1,\n",
        "                 erode_range=1):\n",
        "    \"\"\"Apply variations to a list of images (rotate, width and height shift, scale, erode, dilate)\"\"\"\n",
        "\n",
        "    imgs = imgs.astype(np.float32)\n",
        "    _, h, w = imgs.shape\n",
        "\n",
        "    dilate_kernel = np.ones((int(np.random.uniform(1, dilate_range)),), np.uint8)\n",
        "    erode_kernel = np.ones((int(np.random.uniform(1, erode_range)),), np.uint8)\n",
        "    height_shift = np.random.uniform(-height_shift_range, height_shift_range)\n",
        "    rotation = np.random.uniform(-rotation_range, rotation_range)\n",
        "    scale = np.random.uniform(1 - scale_range, 1)\n",
        "    width_shift = np.random.uniform(-width_shift_range, width_shift_range)\n",
        "\n",
        "    trans_map = np.float32([[1, 0, width_shift * w], [0, 1, height_shift * h]])\n",
        "    rot_map = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)\n",
        "\n",
        "    trans_map_aff = np.r_[trans_map, [[0, 0, 1]]]\n",
        "    rot_map_aff = np.r_[rot_map, [[0, 0, 1]]]\n",
        "    affine_mat = rot_map_aff.dot(trans_map_aff)[:2, :]\n",
        "\n",
        "    for i in range(len(imgs)):\n",
        "        imgs[i] = cv2.warpAffine(imgs[i], affine_mat, (w, h), flags=cv2.INTER_NEAREST, borderValue=255)\n",
        "        imgs[i] = cv2.erode(imgs[i], erode_kernel, iterations=1)\n",
        "        imgs[i] = cv2.dilate(imgs[i], dilate_kernel, iterations=1)\n",
        "\n",
        "    return imgs\n",
        "\n",
        "\n",
        "def normalization(imgs):\n",
        "    \"\"\"Normalize list of images\"\"\"\n",
        "\n",
        "    imgs = np.asarray(imgs).astype(np.float32)\n",
        "    _, h, w = imgs.shape\n",
        "\n",
        "    for i in range(len(imgs)):\n",
        "        m, s = cv2.meanStdDev(imgs[i])\n",
        "        imgs[i] = imgs[i] - m[0][0]\n",
        "        imgs[i] = imgs[i] / s[0][0] if s[0][0] > 0 else imgs[i]\n",
        "\n",
        "    return np.expand_dims(imgs, axis=-1)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Preprocess metodology based in:\n",
        "    H. Scheidl, S. Fiel and R. Sablatnig,\n",
        "    Word Beam Search: A Connectionist Temporal Classification Decoding Algorithm.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def preprocess(img, input_size):\n",
        "    \"\"\"Make the process with the `input_size` to the scale resize\"\"\"\n",
        "    \n",
        "    if isinstance(img, str):\n",
        "        img = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if isinstance(img, tuple):\n",
        "        image, boundbox = img\n",
        "        img = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        for i in range(len(boundbox)):\n",
        "            if isinstance(boundbox[i], float):\n",
        "                total = len(img) if i < 2 else len(img[0])\n",
        "                boundbox[i] = int(total * boundbox[i])\n",
        "\n",
        "        img = np.asarray(img[boundbox[0]:boundbox[1], boundbox[2]:boundbox[3]], dtype=np.uint8)\n",
        "\n",
        "    wt, ht, _ = input_size\n",
        "    h, w = np.asarray(img).shape\n",
        "    f = max((w / wt), (h / ht))\n",
        "\n",
        "    new_size = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1))\n",
        "    img = cv2.resize(img, new_size)\n",
        "    \n",
        "    _, binary = cv2.threshold(img, 254, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Not necessary for the KHATTX dataset, uncomment for IAM or RIMES.\n",
        "    \n",
        "    #if np.sum(img) * 0.8 > np.sum(binary):\n",
        "    #    img = illumination_compensation(img)\n",
        "\n",
        "    #img = remove_cursive_style(img)\n",
        "\n",
        "    target = np.ones([ht, wt], dtype=np.uint8) * 255\n",
        "    target[0:new_size[1], 0:new_size[0]] = img\n",
        "    img = cv2.transpose(target)\n",
        "    img = cv2.flip(img,0)\n",
        "    return img\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Illumination Compensation based in:\n",
        "    K.-N. Chen, C.-H. Chen, C.-C. Chang,\n",
        "    Efficient illumination compensation techniques for text images, in\n",
        "    Digital Signal Processing.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def illumination_compensation(img):\n",
        "    \"\"\"Illumination compensation technique for text image\"\"\"\n",
        "\n",
        "    def scale(img):\n",
        "        s = np.max(img) - np.min(img)\n",
        "        res = img / s\n",
        "        res -= np.min(res)\n",
        "        res *= 255\n",
        "        return res\n",
        "\n",
        "    img = img.astype(np.float32)\n",
        "    height, width = img.shape\n",
        "    sqrt_hw = np.sqrt(height * width)\n",
        "\n",
        "    bins = np.arange(0, 300, 10)\n",
        "    bins[26] = 255\n",
        "    hp = np.histogram(img, bins)\n",
        "    for i in range(len(hp[0])):\n",
        "        if hp[0][i] > sqrt_hw:\n",
        "            hr = i * 10\n",
        "            break\n",
        "\n",
        "    np.seterr(divide='ignore', invalid='ignore')\n",
        "    cei = (img - (hr + 50 * 0.3)) * 2\n",
        "    cei[cei > 255] = 255\n",
        "    cei[cei < 0] = 0\n",
        "\n",
        "    m1 = np.asarray([-1, 0, 1, -2, 0, 2, -1, 0, 1]).reshape((3, 3))\n",
        "    m2 = np.asarray([-2, -1, 0, -1, 0, 1, 0, 1, 2]).reshape((3, 3))\n",
        "    m3 = np.asarray([-1, -2, -1, 0, 0, 0, 1, 2, 1]).reshape((3, 3))\n",
        "    m4 = np.asarray([0, 1, 2, -1, 0, 1, -2, -1, 0]).reshape((3, 3))\n",
        "\n",
        "    eg1 = np.abs(cv2.filter2D(img, -1, m1))\n",
        "    eg2 = np.abs(cv2.filter2D(img, -1, m2))\n",
        "    eg3 = np.abs(cv2.filter2D(img, -1, m3))\n",
        "    eg4 = np.abs(cv2.filter2D(img, -1, m4))\n",
        "\n",
        "    eg_avg = scale((eg1 + eg2 + eg3 + eg4) / 4)\n",
        "\n",
        "    h, w = eg_avg.shape\n",
        "    eg_bin = np.zeros((h, w))\n",
        "    eg_bin[eg_avg >= 30] = 255\n",
        "\n",
        "    h, w = cei.shape\n",
        "    cei_bin = np.zeros((h, w))\n",
        "    cei_bin[cei >= 60] = 255\n",
        "\n",
        "    h, w = eg_bin.shape\n",
        "    tli = 255 * np.ones((h, w))\n",
        "    tli[eg_bin == 255] = 0\n",
        "    tli[cei_bin == 255] = 0\n",
        "\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    erosion = cv2.erode(tli, kernel, iterations=1)\n",
        "    int_img = np.asarray(cei)\n",
        "\n",
        "    estimate_light_distribution(width, height, erosion, cei, int_img)\n",
        "\n",
        "    mean_filter = 1 / 121 * np.ones((11, 11), np.uint8)\n",
        "    ldi = cv2.filter2D(scale(int_img), -1, mean_filter)\n",
        "\n",
        "    result = np.divide(cei, ldi) * 260\n",
        "    result[erosion != 0] *= 1.5\n",
        "    result[result < 0] = 0\n",
        "    result[result > 255] = 255\n",
        "\n",
        "    return np.asarray(result, dtype=np.uint8)\n",
        "\n",
        "\n",
        "def estimate_light_distribution(width, height, erosion, cei, int_img):\n",
        "    \"\"\"Light distribution performed by numba\"\"\"\n",
        "\n",
        "    for y in range(width):\n",
        "        for x in range(height):\n",
        "            if erosion[x][y] == 0:\n",
        "                i = x\n",
        "\n",
        "                while i < erosion.shape[0] and erosion[i][y] == 0:\n",
        "                    i += 1\n",
        "\n",
        "                end = i - 1\n",
        "                n = end - x + 1\n",
        "\n",
        "                if n <= 30:\n",
        "                    h, e = [], []\n",
        "\n",
        "                    for k in range(5):\n",
        "                        if x - k >= 0:\n",
        "                            h.append(cei[x - k][y])\n",
        "\n",
        "                        if end + k < cei.shape[0]:\n",
        "                            e.append(cei[end + k][y])\n",
        "\n",
        "                    mpv_h, mpv_e = max(h), max(e)\n",
        "\n",
        "                    for m in range(n):\n",
        "                        int_img[x + m][y] = mpv_h + (m + 1) * ((mpv_e - mpv_h) / n)\n",
        "\n",
        "                x = end\n",
        "                break\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Deslating image process based in,\n",
        "    A. Vinciarelli and J. Luettin,\n",
        "    A New Normalization Technique for Cursive Handwritten Words.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def remove_cursive_style(img):\n",
        "    \"\"\"Remove cursive writing style from image with deslanting algorithm\"\"\"\n",
        "\n",
        "    def calc_y_alpha(vec):\n",
        "        indices = np.where(vec > 0)[0]\n",
        "        h_alpha = len(indices)\n",
        "\n",
        "        if h_alpha > 0:\n",
        "            delta_y_alpha = indices[h_alpha - 1] - indices[0] + 1\n",
        "\n",
        "            if h_alpha == delta_y_alpha:\n",
        "                return h_alpha * h_alpha\n",
        "        return 0\n",
        "\n",
        "    alpha_vals = [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0]\n",
        "    rows, cols = img.shape\n",
        "    results = []\n",
        "\n",
        "    ret, otsu = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    binary = otsu if ret < 127 else sauvola(img, (int(img.shape[0] / 2), int(img.shape[0] / 2)), 127, 1e-2)\n",
        "\n",
        "    for alpha in alpha_vals:\n",
        "        shift_x = max(-alpha * rows, 0.)\n",
        "        size = (cols + int(np.ceil(abs(alpha * rows))), rows)\n",
        "        transform = np.asarray([[1, alpha, shift_x], [0, 1, 0]], dtype=np.float)\n",
        "\n",
        "        shear_img = cv2.warpAffine(binary, transform, size, cv2.INTER_NEAREST)\n",
        "        sum_alpha = 0\n",
        "        sum_alpha += np.apply_along_axis(calc_y_alpha, 0, shear_img)\n",
        "        results.append([np.sum(sum_alpha), size, transform])\n",
        "\n",
        "    result = sorted(results, key=lambda x: x[0], reverse=True)[0]\n",
        "    warp = cv2.warpAffine(img, result[2], result[1], borderValue=255)\n",
        "\n",
        "    return cv2.resize(warp, dsize=(cols, rows))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Sauvola binarization\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def sauvola(img, window, thresh, k):\n",
        "    \"\"\"Sauvola binarization\"\"\"\n",
        "\n",
        "    rows, cols = img.shape\n",
        "    pad = int(np.floor(window[0] / 2))\n",
        "    sum2, sqsum = cv2.integral2(\n",
        "        cv2.copyMakeBorder(img, pad, pad, pad, pad, cv2.BORDER_CONSTANT))\n",
        "\n",
        "    isum = sum2[window[0]:rows + window[0], window[1]:cols + window[1]] + \\\n",
        "        sum2[0:rows, 0:cols] - \\\n",
        "        sum2[window[0]:rows + window[0], 0:cols] - \\\n",
        "        sum2[0:rows, window[1]:cols + window[1]]\n",
        "\n",
        "    isqsum = sqsum[window[0]:rows + window[0], window[1]:cols + window[1]] + \\\n",
        "        sqsum[0:rows, 0:cols] - \\\n",
        "        sqsum[window[0]:rows + window[0], 0:cols] - \\\n",
        "        sqsum[0:rows, window[1]:cols + window[1]]\n",
        "\n",
        "    ksize = window[0] * window[1]\n",
        "    mean = isum / ksize\n",
        "    std = (((isqsum / ksize) - (mean**2) / ksize) / ksize) ** 0.5\n",
        "    threshold = (mean * (1 + k * (std / thresh - 1))) * (mean >= 100)\n",
        "\n",
        "    return np.asarray(255 * (img >= threshold), 'uint8')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "DeepSpell based text cleaning process.\n",
        "\"\"\"\n",
        "\n",
        "RE_DASH_FILTER = re.compile(r'[\\-\\˗\\֊\\‐\\‑\\‒\\–\\—\\⁻\\₋\\−\\﹣\\－]', re.UNICODE)\n",
        "RE_APOSTROPHE_FILTER = re.compile(r'&#39;|[ʼ՚＇‘’‛❛❜ߴߵ`‵´ˊˋ{}{}{}{}{}{}{}{}{}]'.format(\n",
        "    chr(768), chr(769), chr(832), chr(833), chr(2387),\n",
        "    chr(5151), chr(5152), chr(65344), chr(8242)), re.UNICODE)\n",
        "RE_RESERVED_CHAR_FILTER = re.compile(r'[¶¤«»]', re.UNICODE)\n",
        "RE_LEFT_PARENTH_FILTER = re.compile(r'[\\(\\[\\{\\⁽\\₍\\❨\\❪\\﹙\\（]', re.UNICODE)\n",
        "RE_RIGHT_PARENTH_FILTER = re.compile(r'[\\)\\]\\}\\⁾\\₎\\❩\\❫\\﹚\\）]', re.UNICODE)\n",
        "RE_BASIC_CLEANER = re.compile(r'[^\\w\\s{}]'.format(re.escape(string.punctuation)), re.UNICODE)\n",
        "\n",
        "LEFT_PUNCTUATION_FILTER = \"\"\"!%&),.:;<=>?@\\\\]^_`|}~\"\"\"\n",
        "RIGHT_PUNCTUATION_FILTER = \"\"\"\"(/<=>@[\\\\^_`{|~\"\"\"\n",
        "NORMALIZE_WHITESPACE_REGEX = re.compile(r'[^\\S\\n]+', re.UNICODE)\n",
        "\n",
        "\n",
        "def text_standardize(text):\n",
        "    \"\"\"Organize/add spaces around punctuation marks\"\"\"\n",
        "\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "\n",
        "    text = html.unescape(text).replace(\"\\\\n\", \"\").replace(\"\\\\t\", \"\")\n",
        "\n",
        "    text = RE_RESERVED_CHAR_FILTER.sub(\"\", text)\n",
        "    text = RE_DASH_FILTER.sub(\"-\", text)\n",
        "    text = RE_APOSTROPHE_FILTER.sub(\"'\", text)\n",
        "    text = RE_LEFT_PARENTH_FILTER.sub(\"(\", text)\n",
        "    text = RE_RIGHT_PARENTH_FILTER.sub(\")\", text)\n",
        "    text = RE_BASIC_CLEANER.sub(\"\", text)\n",
        "\n",
        "    text = text.lstrip(LEFT_PUNCTUATION_FILTER)\n",
        "    text = text.rstrip(RIGHT_PUNCTUATION_FILTER)\n",
        "    text = text.translate(str.maketrans({c: \" \"+str(c)+\" \" for c in string.punctuation}))\n",
        "    text = NORMALIZE_WHITESPACE_REGEX.sub(\" \", text.strip())\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def generate_multigrams(sentence):\n",
        "    \"\"\"\n",
        "    Generate n-grams of the sentence.\n",
        "    i.e.:\n",
        "    original sentence: I like code .\n",
        "        > sentence 1 : I like\n",
        "        > sentence 2 : I like code .\n",
        "        > sentence 3 : like\n",
        "        > sentence 4 : like code .\n",
        "        > sentence 5 : code .\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = sentence.split()\n",
        "    tk_length = len(tokens)\n",
        "    multigrams = []\n",
        "\n",
        "    for y in range(tk_length):\n",
        "        new_sentence = True\n",
        "        support_text = \"\"\n",
        "\n",
        "        for x in range(y, tk_length):\n",
        "            if y == 0 and tk_length > 2 and x == (tk_length - 1):\n",
        "                continue\n",
        "\n",
        "            if len(tokens[x]) <= 2 and tokens[x] != tokens[-1]:\n",
        "                support_text += \" \"+str(tokens[x])\n",
        "                continue\n",
        "\n",
        "            last = \"\"\n",
        "            if x > y and len(multigrams) > 0 and not new_sentence:\n",
        "                last = multigrams[-1]\n",
        "\n",
        "            multigrams.append((str(last)+str(support_text)+\" \"+str(tokens[x])).strip())\n",
        "            new_sentence = False\n",
        "            support_text = \"\"\n",
        "\n",
        "    return multigrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8rNQoogSdVb_"
      },
      "source": [
        "## Reader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bLycEeHYczyj",
        "colab": {}
      },
      "source": [
        "\"\"\"Dataset reader and process\"\"\"\n",
        "\n",
        "import os\n",
        "import html\n",
        "import string\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from functools import partial\n",
        "from glob import glob\n",
        "from multiprocessing import Pool\n",
        "\n",
        "\n",
        "class Dataset():\n",
        "    \"\"\"Dataset class to read images and sentences from base (raw files)\"\"\"\n",
        "\n",
        "    def __init__(self, source, name):\n",
        "        self.source = source\n",
        "        self.name = name\n",
        "        self.dataset = None\n",
        "        self.partitions = ['train', 'valid', 'test']\n",
        "\n",
        "    def read_partitions(self):\n",
        "        \"\"\"Read images and sentences from dataset\"\"\"\n",
        "\n",
        "        self.dataset = getattr(self, \"_\"+str(self.name))()\n",
        "\n",
        "    def preprocess_partitions(self, input_size):\n",
        "        \"\"\"Preprocess images and sentences from partitions\"\"\"\n",
        "\n",
        "        for y in self.partitions:\n",
        "            arange = range(len(self.dataset[y]['gt']))\n",
        "            for i in reversed(arange):\n",
        "                text = text_standardize(self.dataset[y]['gt'][i])\n",
        "                if not self.check_text(text):\n",
        "                    self.dataset[y]['gt'].pop(i)\n",
        "                    self.dataset[y]['dt'].pop(i)\n",
        "                    continue\n",
        "                self.dataset[y]['gt'][i] = text.encode()\n",
        "            print(\"pool\")\n",
        "            pool = Pool()\n",
        "            print(\"selfie\")\n",
        "            self.dataset[y]['dt'] = pool.map(partial(preprocess, input_size=input_size), self.dataset[y]['dt'])\n",
        "            print(\"close\")\n",
        "            pool.close()\n",
        "            print(\"join\")\n",
        "            pool.join()\n",
        "\n",
        "    def _iam(self):\n",
        "        \"\"\"IAM dataset reader\"\"\"\n",
        "\n",
        "        pt_path = os.path.join(self.source, \"largeWriterIndependentTextLineRecognitionTask\")\n",
        "        paths = {\"train\": open(os.path.join(pt_path, \"trainset.txt\")).read().splitlines(),\n",
        "                 \"valid\": open(os.path.join(pt_path, \"validationset1.txt\")).read().splitlines(),\n",
        "                 \"test\": open(os.path.join(pt_path, \"testset.txt\")).read().splitlines()}\n",
        "\n",
        "        lines = open(os.path.join(self.source, \"ascii\", \"lines.txt\")).read().splitlines()\n",
        "        gt_dict = dict()\n",
        "\n",
        "        for line in lines:\n",
        "            if (not line or line[0] == \"#\"):\n",
        "                continue\n",
        "\n",
        "            splitted = line.split()\n",
        "\n",
        "            if splitted[1] == \"ok\":\n",
        "                gt_dict[splitted[0]] = \" \".join(splitted[8::]).replace(\"|\", \" \")\n",
        "\n",
        "        dataset = dict()\n",
        "\n",
        "        for i in self.partitions:\n",
        "            dataset[i] = {\"dt\": [], \"gt\": []}\n",
        "\n",
        "            for line in paths[i]:\n",
        "                try:\n",
        "                    split = line.split(\"-\")\n",
        "                    folder = str(split[0])+\" \"+str(split[1])\n",
        "\n",
        "                    img_file = str(split[0])+\"-\"+str(split[1])+\"-\"+str(split[2])+\".png\"\n",
        "                    img_path = os.path.join(self.source, \"lines\", split[0], folder, img_file)\n",
        "\n",
        "                    dataset[i]['gt'].append(gt_dict[line])\n",
        "                    dataset[i]['dt'].append(img_path)\n",
        "                except KeyError:\n",
        "                    pass\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def _rimes(self):\n",
        "        \"\"\"Rimes dataset reader\"\"\"\n",
        "\n",
        "        def generate(xml, subpath, paths, validation=False):\n",
        "            xml = ET.parse(os.path.join(self.source, xml)).getroot()\n",
        "            dt = []\n",
        "\n",
        "            for page_tag in xml:\n",
        "                page_path = page_tag.attrib['FileName']\n",
        "\n",
        "                for i, line_tag in enumerate(page_tag.iter(\"Line\")):\n",
        "                    text = html.unescape(line_tag.attrib['Value'])\n",
        "                    text = \" \".join(text.split())\n",
        "\n",
        "                    bound = [abs(int(line_tag.attrib['Top'])), abs(int(line_tag.attrib['Bottom'])),\n",
        "                             abs(int(line_tag.attrib['Left'])), abs(int(line_tag.attrib['Right']))]\n",
        "                    dt.append([os.path.join(subpath, page_path), text, bound])\n",
        "\n",
        "            if validation:\n",
        "                index = int(len(dt) * 0.9)\n",
        "                paths['valid'] = dt[index:]\n",
        "                paths['train'] = dt[:index]\n",
        "            else:\n",
        "                paths['test'] = dt\n",
        "\n",
        "        dataset, paths = dict(), dict()\n",
        "        generate(\"training_2011.xml\", \"training_2011\", paths, validation=True)\n",
        "        generate(\"eval_2011_annotated.xml\", \"eval_2011\", paths, validation=False)\n",
        "\n",
        "        for i in self.partitions:\n",
        "            dataset[i] = {\"dt\": [], \"gt\": []}\n",
        "\n",
        "            for item in paths[i]:\n",
        "                boundbox = [item[2][0], item[2][1], item[2][2], item[2][3]]\n",
        "                dataset[i]['dt'].append((os.path.join(self.source, item[0]), boundbox))\n",
        "                dataset[i]['gt'].append(item[1])\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def _khatt(self):\n",
        "      \n",
        "        \"\"\"KHATT dataset reader\"\"\"\n",
        "        \n",
        "        symbol_dict = {'hh': \"ء\",'am':\"آ\",'ae': \"أ\",'ah': \"إ\",'aa': \"ا\",'ba': \"ب\",\n",
        "               'ta': \"ت\",'teE': \"ة\",'th': \"ث\",'ja': \"ج\",'ha': \"ح\",'kh': \"خ\",\n",
        "               'da': \"د\",'dh': \"ذ\",'ra': \"ر\",'za': \"ز\",'se': \"س\",'sh': \"ش\",\n",
        "               'sa': \"ص\",'de': \"ض\",'to': \"ط\",'zha': \"ظ\",'ay': \"ع\",'gh': \"غ\",\n",
        "               'fa': \"ف\",'ka': \"ق\",'ke': \"ك\",'la': \"ل\",'ma': \"م\",'na': \"ن\",\n",
        "               'he': \"ه\",'wa': \"و\",'wl': \"ؤ\",'ya': \"ي\",'ee': \"ى\",'al': \"ئ\",\n",
        "               'n0': \"0\",'n1': \"1\",'n2': \"2\",'n3': \"3\",'n4': \"4\",'n5': \"5\",\n",
        "               'n6': \"6\",'n7': \"7\",'n8': \"8\",'n9': \"9\",'atr': \"@\",'col': \":\",\n",
        "               'dbq': '\"','com': \",\",'qts': \"?\",'exc': \"!\",'dot': \".\",'bro': \"(\",\n",
        "               'brc': \")\",'fsl': \"/\",'bsl': \"\\\\\",'equ': \"=\",'hyp': \"-\",'usc': \"_\",\n",
        "               'src': \"#\",'per': \"%\", 'sp': \" \", 'te': \"ة\"\n",
        "              }\n",
        "        def to_symbs(inp):\n",
        "            temp = inp.split()\n",
        "            out = \"\"\n",
        "            for c in temp:\n",
        "                out += symbol_dict[c]\n",
        "            out.replace(\" \",\"\")\n",
        "            return out\n",
        "        pt_path = os.path.join(self.source, \"khatt\") # os.path.join(self.source, \"sets\")\n",
        "        \n",
        "        paths = {\"train\": open(os.path.join(pt_path, \"Training-Groundtruth.txt\")).read().splitlines(),\n",
        "                 \"valid\": open(os.path.join(pt_path, \"Validation-Groundtruth.txt\")).read().splitlines(),\n",
        "                 \"test\": open(os.path.join(pt_path, \"Test-Groundtruth.txt\")).read().splitlines()}\n",
        "        \n",
        "        image_paths = {\"train\": os.path.join(pt_path, \"Training\"),\n",
        "                     \"valid\": os.path.join(pt_path, \"Validation\"),\n",
        "                     \"test\": os.path.join(pt_path, \"Test\")}\n",
        "        \n",
        "        dataset = dict()\n",
        "        \n",
        "        for i in self.partitions:\n",
        "            dataset[i] = {\"dt\": [], \"gt\": []}\n",
        "            image_path = image_paths[i]\n",
        "            img_list = sorted(os.listdir(image_path))\n",
        "            txt_file = paths[i]\n",
        "            j = 0\n",
        "            for img in img_list:\n",
        "                dataset[i]['dt'].append(os.path.join(image_path, img))\n",
        "                line = txt_file[j].replace(img,\"\").replace(\"\t \",\"\")\n",
        "                line = to_symbs(line)\n",
        "                #print(line)\n",
        "                dataset[i]['gt'].append(line)\n",
        "                j+=1\n",
        "        return dataset\n",
        "    def _khattx(self):\n",
        "        \n",
        "        \"\"\"KHATTX dataset reader\"\"\"\n",
        "        \n",
        "        symbol_dict = {'hh': \"ء\",'am':\"آ\",'ae': \"أ\",'ah': \"إ\",'aa': \"ا\",'ba': \"ب\",\n",
        "               'ta': \"ت\",'teE': \"ة\",'th': \"ث\",'ja': \"ج\",'ha': \"ح\",'kh': \"خ\",\n",
        "               'da': \"د\",'dh': \"ذ\",'ra': \"ر\",'za': \"ز\",'se': \"س\",'sh': \"ش\",\n",
        "               'sa': \"ص\",'de': \"ض\",'to': \"ط\",'zha': \"ظ\",'ay': \"ع\",'gh': \"غ\",\n",
        "               'fa': \"ف\",'ka': \"ق\",'ke': \"ك\",'la': \"ل\",'ma': \"م\",'na': \"ن\",\n",
        "               'he': \"ه\",'wa': \"و\",'wl': \"ؤ\",'ya': \"ي\",'ee': \"ى\",'al': \"ئ\",\n",
        "               'n0': \"0\",'n1': \"1\",'n2': \"2\",'n3': \"3\",'n4': \"4\",'n5': \"5\",\n",
        "               'n6': \"6\",'n7': \"7\",'n8': \"8\",'n9': \"9\",'atr': \"@\",'col': \":\",\n",
        "               'dbq': '\"','com': \",\",'qts': \"?\",'exc': \"!\",'dot': \".\",'bro': \"(\",\n",
        "               'brc': \")\",'fsl': \"/\",'bsl': \"\\\\\",'equ': \"=\",'hyp': \"-\",'usc': \"_\",\n",
        "               'scr': \"#\",'per': \"%\", 'sp': \" \", 'te': \"ة\"\n",
        "              }\n",
        "        def to_symbs(inp):\n",
        "            temp = inp.split()\n",
        "            out = \"\"\n",
        "            for c in temp:\n",
        "                out += symbol_dict[c]\n",
        "            out.replace(\" \",\"\")\n",
        "            return out\n",
        "        pt_path = os.path.join(self.source, \"KHATTX\") # os.path.join(self.source, \"sets\")\n",
        "        \n",
        "        paths = {\"train\": open(os.path.join(pt_path, \"Training-Groundtruth.txt\")).read().splitlines(),\n",
        "                 \"valid\": open(os.path.join(pt_path, \"Validation-Groundtruth.txt\")).read().splitlines(),\n",
        "                 \"test\": open(os.path.join(pt_path, \"Test-Groundtruth.txt\")).read().splitlines()}\n",
        "        \n",
        "        image_paths = {\"train\": os.path.join(pt_path, \"Training\"),\n",
        "                     \"valid\": os.path.join(pt_path, \"Validation\"),\n",
        "                     \"test\": os.path.join(pt_path, \"Test\")}\n",
        "        \n",
        "        dataset = dict()\n",
        "        \n",
        "        for i in self.partitions:\n",
        "            dataset[i] = {\"dt\": [], \"gt\": []}\n",
        "            image_path = image_paths[i]\n",
        "            img_list = sorted(os.listdir(image_path))\n",
        "            txt_file = sorted(paths[i])\n",
        "            j = 0\n",
        "            JJ = 0\n",
        "            for img in img_list:\n",
        "                found = False\n",
        "                while j<len(txt_file):\n",
        "                    line = txt_file[j]\n",
        "                    if(line.find(img)!=-1):\n",
        "                        found = True\n",
        "                        break\n",
        "                    j+=1\n",
        "                if not found:\n",
        "                    print(img,i)\n",
        "                    j = JJ\n",
        "                    continue\n",
        "                line = line.replace(img,\"\")\n",
        "                line = to_symbs(line)\n",
        "                dataset[i]['dt'].append(os.path.join(image_path, img))\n",
        "                dataset[i]['gt'].append(line)\n",
        "                j+=1\n",
        "                JJ = j\n",
        "        \"\"\"for pt in self.partitions:\n",
        "            print(pt,\": \",len(dataset[pt]['gt']),\" orig: \",len(os.listdir(image_paths[pt])))\n",
        "        def show_example(im,label):\n",
        "            print(label[0])\n",
        "            img = cv2.imread(im, cv2.IMREAD_GRAYSCALE)\n",
        "            cv2_imshow(img)\n",
        "            cv2_imshow(preprocess(im,input_size = (1024, 64, 1)))\n",
        "        show_example(dataset[self.partitions[0]]['dt'][0],dataset[self.partitions[0]]['gt'][0])\"\"\"\n",
        "        return dataset\n",
        "    \n",
        "    @staticmethod\n",
        "    def check_text(text):\n",
        "        \"\"\"Make sure text has more characters instead of punctuation marks\"\"\"\n",
        "\n",
        "        strip_punc = text.strip(string.punctuation).strip()\n",
        "        no_punc = text.translate(str.maketrans(\"\", \"\", string.punctuation)).strip()\n",
        "\n",
        "        if len(text) == 0 or len(strip_punc) == 0 or len(no_punc) == 0:\n",
        "            return False\n",
        "\n",
        "        punc_percent = (len(strip_punc) - len(no_punc)) / len(strip_punc)\n",
        "\n",
        "        return len(no_punc) >= 2 and punc_percent <= 0.1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B8G0l74od50G",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}